{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"17d5c4fa-0e12-483c-a9dc-a0da2cd10381","showTitle":false,"title":""}},"outputs":[],"source":["%run config"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Depedencies \n","from pyspark.sql.functions import * "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b613fa63-adb9-4938-b65b-4eb3631a5823","showTitle":false,"title":""}},"outputs":[],"source":["#API call to grab all of the acitivites within a personal account\n","def activity_api_call(access_token):\n","    activites_url = \"https://www.strava.com/api/v3/athlete/activities\"\n","    header = {'Authorization': 'Bearer ' + access_token}\n","    param = {'per_page': 200, 'page': 1}\n","    activity_dataset = requests.get(activites_url, headers=header, params=param).json()\n","    \n","    return activity_dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6d9ca4f7-85af-4730-9cc0-d534ef0a9221","showTitle":false,"title":""}},"outputs":[],"source":["my_dataset = activity_api_call(access_token)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5ae46e42-7f48-4a1b-8115-8e815367285d","showTitle":false,"title":""}},"outputs":[],"source":["#get all of the activity ids from the bigger dataset\n","#xtract all of the columns we want\n","def extract_activities(dataset):\n","    \"\"\"Function to seperate activity_ids and create an activity dataframe. Returns a df of only the activity ids, and another df with more details about the activiity. \"\"\"\n","    activity_ids = []\n","    start_date = []\n","    activity_name =[]\n","    distance = []\n","    moving_time = []\n","    elapsed_time = []\n","    sport_type = []\n","    total_elevation_gain =[]\n","    count = 0\n","    while count < len(dataset):\n","        activity_ids.append(dataset[count]['id'])\n","        start_date.append(dataset[count]['start_date'])\n","        activity_name.append(dataset[count]['name'])\n","        distance.append(dataset[count]['distance'])\n","        moving_time.append(dataset[count]['moving_time'])\n","        elapsed_time.append(dataset[count]['elapsed_time'])\n","        sport_type.append(dataset[count]['sport_type'])\n","        total_elevation_gain.append(dataset[count]['total_elevation_gain'])\n","        count += 1 \n","        \n","    #convert list to dataframe   \n","    from pyspark.sql.types import LongType\n","    activity_id_DF = spark.createDataFrame(activity_ids, LongType())\n","    activity_id_DF = activity_id_DF.withColumnRenamed('value', 'activity_id')\\\n","                  .withColumn(\"ingest_file_name\", lit(\"activity_ids\")) \\\n","                  .withColumn(\"ingested_at\", lit(current_timestamp()))\n","    \n","    #columns names for initial DF\n","    #need to specify schema\n","    columns = ['activity_ids','start_date', 'activity_name', 'distance', 'moving_time','elapsed_time', 'sport_type'\\\n","          ,'total_elevation_gain']\n","    #list of lists\n","    extracted_data = [activity_ids,start_date, activity_name, distance, moving_time,elapsed_time, sport_type\\\n","          ,total_elevation_gain]\n","\n","    import pandas as pd\n","    #create a pandas Dataframe, then convert to spark to write to storage\n","    pdf = pd.DataFrame.from_dict(dict(zip(columns, extracted_data)))\n","    activity_df = spark.createDataFrame(pdf)\n","\n","    activity_df = activity_df.withColumn(\"ingest_file_name\", lit(\"activity_information\")) \\\n","                             .withColumn(\"ingested_at\", lit(current_timestamp()))\n","\n","    return activity_id_DF, activity_df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2459ffb8-1a9a-4a03-92fa-a7c86cac5fcf","showTitle":false,"title":""}},"outputs":[],"source":["activity_id_DF, activity_df = extract_activities(my_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9d4f172f-5051-4d93-9057-e92313ca31c3","showTitle":false,"title":""}},"outputs":[],"source":["#write the activity ids to storage, overwrite the previous iteration\n","def write_dataframe_to_storage(dataset, storage_path, option, mode ):\n","    \"\"\"Function to write activity ids to storage. Will overwrite current delta file in storage\n","    Option refers to schema overwriteSchema or mergeSchema, mode being either overwrite or append\"\"\"\n","    dataset.write.format(\"delta\")\\\n","    .option(option, \"true\")\\\n","    .mode(mode)\\\n","    .save(storage_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ff90bbb6-be4b-462e-953f-bebff06fcbd8","showTitle":false,"title":""}},"outputs":[],"source":["write_dataframe_to_storage(activity_id_DF,activity_id_path, \"overwriteSchema\",\"overwrite\" )"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"38b7d57a-e371-40b5-827d-cdd37122276b","showTitle":false,"title":""}},"outputs":[],"source":["#read the activities from storage\n","stored_activity_ids = spark.read.format(\"delta\").load(activity_id_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#read in historical activity IDS to compare\n","#trying to compare current query vs historical, if nothing is written to storage ie first run, still need to execute and write original DF to storage\n","def get_historical_acitivies(storagepath, historical_df_to_write):\n","    \"\"\"Retrieve record from file path, if nothing exists, insert df to write to storage\"\"\"\n","    from pyspark.sql.utils import AnalysisException\n","    try:\n","        #try to read from storage\n","        historical_dataframe = spark.read.format(\"delta\").load(historical_activity_id_path)\n","    except: \n","        try:\n","        #if that fails, write the current dataframe to storage\n","            write_dataframe_to_storage(historical_df_to_write,storagepath, \"mergeSchema\", \"append\" )\n","        finally:\n","            historical_dataframe = spark.read.format(\"delta\").load(storagepath)\n","\n","    return historical_dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["historical_activites.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["historical_activites = get_historical_acitivies(historical_activity_id_path, activity_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["activity_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Extract activity ids from dataframes to list, to make comparison \n","#activity_id_list = activity_id_DF.select('activity_ids').distinct().collect()\n","activity_id_list = stored_activity_ids.select('activity_id').distinct().rdd.flatMap(lambda x: x).collect()\n","\n","historical_activity_id_list = historical_activites.select('activity_ids').distinct().rdd.flatMap(lambda x: x).collect()\n","\n","activity_ids_not_written_to_storage = [x for x in activity_id_list if x not in historical_activity_id_list ]\n","\n","new_activities = activity_df.filter(activity_df.activity_ids.isin(activity_ids_not_written_to_storage))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["write_dataframe_to_storage(new_activities, historical_activity_id_path,\"mergeSchema\", \"append\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_activity_dataset = spark.read.format(\"delta\").load(historical_activity_id_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["full_activity_dataset.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#take ones not in storage, and grab them from activity df, and write those to storage"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#need to compare distinct values and then append those to storage\n","def compare_current_vs_historical():\n","    \"\"\"Compares historical activity IDS already in storage, vs the most recent API query\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["historical_activity_id_path\n","new_activity_ids = [x for x in activity_id_DF['activity_ids'] if x not in historical_activity_ids['activity_id'] ]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"70a2a695-2496-492d-b69d-6e5e551b7666","showTitle":false,"title":""}},"outputs":[],"source":["activity_df.filter(df.acitivty_id.isin(activity_ids_not_written_to_storage))"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Query_Activities","notebookOrigID":4431027783720630,"widgets":{}},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"89bbb57337a288069efe3ede2e44e349d48d03d33172adbe5738fcfdbda01bd0"}}},"nbformat":4,"nbformat_minor":0}
